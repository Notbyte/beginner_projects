{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2cTircjM-O7",
        "outputId": "1f702c3b-74c9-4d41-a346-c4e7ac77d2e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching data: 100%|██████████| 69/69 [01:07<00:00,  1.01it/s]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "\n",
        "def slugify(text):\n",
        "    \"\"\"\n",
        "    Convert a string to a slug by:\n",
        "    - Removing diacritics (e.g., converting 'Đà Nẵng' to 'Da Nang')\n",
        "    - Lowercasing\n",
        "    - Replacing non-alphanumeric characters with hyphens\n",
        "    \"\"\"\n",
        "    # Normalize to remove accents\n",
        "    # Replace special characters Đ and đ with D and d\n",
        "    text = text.replace('Đ', 'D').replace('đ', 'd')\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    text = text.lower()\n",
        "    # Replace any non-alphanumeric characters with hyphens and remove extra hyphens\n",
        "    text = re.sub(r'[^a-z0-9]+', '-', text).strip('-')\n",
        "    return text\n",
        "\n",
        "\n",
        "# Function to fetch data from the API\n",
        "def fetch_data(token, type_list, range_start, range_end):\n",
        "    results = []  # List to store each record as a dictionary\n",
        "    url = \"https://internal-vroute-cmc.vexere.com/v1/goyolo/area/\"\n",
        "    # Provided Bearer token\n",
        "\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\"\n",
        "    }\n",
        "    for i in tqdm(range(range_start,range_end), desc=\"Fetching data\"):\n",
        "        name = None\n",
        "        slug = None\n",
        "        url = f\"https://internal-vroute-cmc.vexere.com/v1/goyolo/area/{str(i)}\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "        # Check if the HTTP request was successful\n",
        "        if response.status_code == 401:\n",
        "            print(\"Token expired\")\n",
        "            return None\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if data['data']['type'] not in type_list:\n",
        "                continue\n",
        "            # Check if the returned message is \"success\"\n",
        "            if data.get(\"message\") == \"success\":\n",
        "              try:\n",
        "                # Extract and print the name from the \"data\" field\n",
        "                name = data.get(\"data\", {}).get(\"name\")\n",
        "                # Create a slug from the name, if available.\n",
        "                slug = slugify(name)\n",
        "                results.append({\"id\": i, \"name\": name, \"slug\": slug})\n",
        "              except Exception as e:\n",
        "                continue\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    a = pd.DataFrame(results)\n",
        "    a.to_csv(\"mapping.csv\", index=False)\n",
        "\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0eXAiOjIsInVzciI6ImZlIiwiY2lkIjoiYTRlYWM1MDAtMzYyNC0xMWU1LWFjOWUtMDkxMjRjNjAxMDEzIiwiZXhwIjoxNzQxMzYzODk0fQ.HuItYy9aH6txxwsOmG63IQbrp8keO-keBrQmOOb9TVw\"\n",
        "    type_list = [3]\n",
        "    fetch_data(token, type_list, 1, 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mục mới"
      ],
      "metadata": {
        "id": "TSZltYJlOrPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mục mới"
      ],
      "metadata": {
        "id": "kxfcdIZjOjgv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd-qkvtgM-O9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def slugify(text):\n",
        "    \"\"\"\n",
        "    Convert a string to a slug:\n",
        "    - Remove diacritics (e.g., 'Đà Nẵng' → 'Da Nang')\n",
        "    - Convert to lowercase\n",
        "    - Replace non-alphanumeric characters with hyphens\n",
        "    \"\"\"\n",
        "    text = text.replace('Đ', 'D').replace('đ', 'd')\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    text = text.lower()\n",
        "    return re.sub(r'[^a-z0-9]+', '-', text).strip('-')\n",
        "\n",
        "\n",
        "def format_api_date(date_str):\n",
        "    \"\"\"\n",
        "    Convert a date from 'DD-MM-YYYY' format to the API format: 'YYYY-MM-DDT00:00:00+07:00'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dt = datetime.strptime(date_str, \"%d-%m-%Y\")\n",
        "        return dt.strftime(\"%Y-%m-%dT00:00:00+07:00\")\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Invalid date format. Use 'DD-MM-YYYY'.\")\n",
        "\n",
        "\n",
        "def get_bus_trip_count(url):\n",
        "\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed to retrieve the page.\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Find the element with class \"text-result-number\"\n",
        "    result_element = soup.find(class_=\"text-result-number\")\n",
        "\n",
        "    if result_element:\n",
        "        # Get the text and strip any surrounding whitespace\n",
        "        count_text = result_element.get_text(strip=True)\n",
        "        count_text = count_text.split()\n",
        "        return count_text[0]\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def get_total(from_destination, to_destination, date=\"27-02-2025\", mapping_csv=\"mapping.csv\"):\n",
        "    \"\"\"\n",
        "    Given departure and destination names, enrich the data by:\n",
        "      - Reading the mapping CSV (which has columns: id, name, slug),\n",
        "      - Normalizing the destination names using slugify,\n",
        "      - Building the route code and URL,\n",
        "      - Fetching the total bus trip count from the URL.\n",
        "\n",
        "    Returns a DataFrame with columns: from_name, to_name, total_trip, date.\n",
        "    \"\"\"\n",
        "    # Read the mapping CSV into a DataFrame.\n",
        "    city_df = pd.read_csv(mapping_csv)\n",
        "\n",
        "    # Normalize the destination names using slugify.\n",
        "    normalized_from = slugify(from_destination)\n",
        "    normalized_to = slugify(to_destination)\n",
        "\n",
        "    # Look up the corresponding rows in the mapping CSV.\n",
        "    row_from = city_df[city_df['slug'] == normalized_from]\n",
        "    row_to = city_df[city_df['slug'] == normalized_to]\n",
        "\n",
        "    if row_from.empty or row_to.empty:\n",
        "        print(\"Error: One or both destinations not found in mapping CSV.\")\n",
        "        return None\n",
        "\n",
        "    # Extract required fields.\n",
        "    from_id = row_from.iloc[0]['id']\n",
        "    from_slug = row_from.iloc[0]['slug']\n",
        "    to_id = row_to.iloc[0]['id']\n",
        "    to_slug = row_to.iloc[0]['slug']\n",
        "\n",
        "    # Generate the route code.\n",
        "    route_code = f\"1{from_id}t1{to_id}1\"\n",
        "\n",
        "    # Build the URL.\n",
        "    url = (\n",
        "        f\"https://vexere.com/vi-VN/ve-xe-khach-tu-{from_slug}-di-{to_slug}-\"\n",
        "        f\"{route_code}.html?date={date}\"\n",
        "    )\n",
        "\n",
        "    # Fetch the total trip count using the provided function.\n",
        "    total_trip = get_bus_trip_count(url)\n",
        "\n",
        "    return total_trip\n",
        "\n",
        "\n",
        "\n",
        "def get_city_id(city_df, city_name):\n",
        "    \"\"\"\n",
        "    Retrieve the city ID from the mapping CSV based on the city name.\n",
        "    \"\"\"\n",
        "    city_row = city_df[city_df['slug'] == slugify(city_name)]\n",
        "    if not city_row.empty:\n",
        "        return city_row.iloc[0, 0]  # Assuming city ID is in the first column\n",
        "    raise ValueError(f\"City '{city_name}' not found in the mapping file.\")\n",
        "\n",
        "\n",
        "\n",
        "def fetch_bus_data(token, from_id, to_id, date):\n",
        "    \"\"\"\n",
        "    Fetch bus data from the API for a given route and date.\n",
        "    Implements pagination to retrieve more than 100 records.\n",
        "    \"\"\"\n",
        "    api_url = \"https://internal-vroute-cmc.vexere.com/v2/route\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "    all_data = []\n",
        "    page = 1\n",
        "    pagesize = 100000  # adjust if needed\n",
        "    while True:\n",
        "        query_params = {\n",
        "            \"filter[from]\": from_id,\n",
        "            \"filter[to]\": to_id,\n",
        "            \"filter[date]\": format_api_date(date),\n",
        "            \"filter[online_ticket]\": 0,\n",
        "            \"filter[is_promotion]\": 0,\n",
        "            \"filter[covid_utility]\": 0,\n",
        "            \"filter[speaking_english_utility]\": 0,\n",
        "            \"filter[enabled_gps]\": 0,\n",
        "            \"filter[has_cop]\": 0,\n",
        "            \"filter[online_reserved]\": 0,\n",
        "            \"filter[suggestion]\": \"DEFAULT\",\n",
        "            \"filter[fare][min]\": 0,\n",
        "            \"filter[fare][max]\": 2000000,\n",
        "            \"filter[available_seat][min]\": 1,\n",
        "            \"filter[available_seat][max]\": 50,\n",
        "            \"filter[rating][min]\": 0,\n",
        "            \"filter[rating][max]\": 5,\n",
        "            \"filter[limousine]\": 0,\n",
        "            \"filter[has_unfixed_point]\": 0,\n",
        "            \"page\": page,\n",
        "            \"pagesize\": pagesize\n",
        "        }\n",
        "\n",
        "        response = requests.get(api_url, headers=headers, params=query_params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve data for {date} on page {page}. Status Code: {response.status_code}\")\n",
        "            print(response.text)\n",
        "            break\n",
        "\n",
        "        data = response.json().get('data', [])\n",
        "        if not data:\n",
        "            # No more records to fetch\n",
        "            break\n",
        "\n",
        "        all_data.extend(data)\n",
        "        page += 1\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "# def process_bus_data(data):\n",
        "#     \"\"\"\n",
        "#     Process the API response data and count occurrences of each bus company.\n",
        "#     \"\"\"\n",
        "#     company_list = [route['company']['name'] for route in data if 'company' in route]\n",
        "#     return Counter(company_list)\n",
        "\n",
        "\n",
        "\n",
        "def process_bus_data(data):\n",
        "    \"\"\"\n",
        "    Process the API response data and, for each company, collect the unique available seat counts.\n",
        "\n",
        "    It assumes that each record in `data` contains a 'company' key and that the available seat count is\n",
        "    found in:\n",
        "        company['available_seat_info']['seat_type']['1']['total_available_seat']\n",
        "    as well as optionally in the 'seat_group' section.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping each company name to a set of unique available seat counts.\n",
        "    \"\"\"\n",
        "    company_seat_counts = defaultdict(set)\n",
        "\n",
        "    for record in data:\n",
        "        company = record['company']['name']\n",
        "        if 'available_seat_info' not in record:\n",
        "            continue\n",
        "        seat_info = record['available_seat_info']['seat_type'].keys()\n",
        "        for key in seat_info:\n",
        "            seat_info = record['available_seat_info']['seat_type'][key]['total_available_seat']\n",
        "            company_seat_counts[company].add(seat_info)\n",
        "\n",
        "    seat_counts = {company: len(seats) for company, seats in company_seat_counts.items()}\n",
        "    return seat_counts\n",
        "\n",
        "\n",
        "def group_and_sum(dataframe):\n",
        "    \"\"\"\n",
        "    Groups the DataFrame by 'Company Name' and sums the 'Count' for each group.\n",
        "\n",
        "    Parameters:\n",
        "        dataframe (pd.DataFrame): The input DataFrame with columns 'Company Name' and 'Count'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with each company and the corresponding summed count.\n",
        "    \"\"\"\n",
        "    return dataframe.groupby('Company Name', as_index=False)['Count'].sum()\n",
        "\n",
        "\n",
        "def get_bus_count(token, from_destination, to_destination, dates=[\"27-02-2025\"], mapping_csv=\"mapping.csv\"):\n",
        "    \"\"\"\n",
        "    Get the number of bus trips from a source to a destination for multiple dates.\n",
        "    Saves the result as a CSV in the \"result\" directory.\n",
        "\n",
        "    :param token: API authentication token\n",
        "    :param from_destination: Departure city\n",
        "    :param to_destination: Destination city\n",
        "    :param dates: List of dates (default: [\"27-02-2025\"])\n",
        "    :param mapping_csv: Path to the city mapping CSV file\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    # Load city mapping file\n",
        "    try:\n",
        "        city_df = pd.read_csv(mapping_csv)\n",
        "        from_id = get_city_id(city_df, from_destination)\n",
        "        to_id = get_city_id(city_df, to_destination)\n",
        "    except (FileNotFoundError, ValueError) as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Ensure result directory exists\n",
        "    result_dir = \"result\"\n",
        "    os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "    all_data = []\n",
        "    total_count = 0\n",
        "\n",
        "    for date in dates:\n",
        "        data = fetch_bus_data(token, from_id, to_id, date)\n",
        "        company_counts = process_bus_data(data)\n",
        "        # print(company_counts)\n",
        "        if data == []:\n",
        "            # print(f\"No data available from {from_destination} to {to_destination} in {date}.\")\n",
        "            continue\n",
        "        if not company_counts:\n",
        "            # print(f\"No data available from {from_destination} to {to_destination} in {date}.\")\n",
        "            continue\n",
        "\n",
        "        df_counts = pd.DataFrame(sorted(company_counts.items(), key=lambda x: x[1], reverse=True),\n",
        "                                 columns=[\"Company Name\", \"Count\"])\n",
        "        total_count += df_counts[\"Count\"].sum()\n",
        "        all_data.append(df_counts)\n",
        "\n",
        "    if all_data == []:\n",
        "        return None\n",
        "    # Merge all dates into one DataFrame\n",
        "    if all_data:\n",
        "        final_df = pd.concat(all_data, ignore_index=True)\n",
        "        total_row = pd.DataFrame([[\"Total\", total_count]], columns=[\"Company Name\", \"Count\"])\n",
        "        group_and_sum_df = group_and_sum(final_df)\n",
        "        group_and_sum_df['routes'] = f\"{from_destination} - {to_destination}\"\n",
        "        group_and_sum_df['date'] = f\"{date}\"\n",
        "\n",
        "        df_final = pd.concat([final_df, total_row], ignore_index=True)\n",
        "        # Save results to CSV\n",
        "        final_file = f\"{result_dir}/{from_destination}_{to_destination}_all_dates.csv\"\n",
        "        df_final.to_csv(final_file, index=False)\n",
        "        # print(f\"All data saved to {final_file}\")\n",
        "    else:\n",
        "        print(\"No data to save.\")\n",
        "\n",
        "    return from_destination, to_destination, total_count, group_and_sum_df\n",
        "\n",
        "\n",
        "def get_best(df, top = 20):\n",
        "    # Create a normalized route column by sorting the two destinations\n",
        "    df['route'] = df.apply(\n",
        "        lambda row: tuple(sorted([row['from destination'], row['to destination']])), axis=1\n",
        "    )\n",
        "    # For each route group, get the index of the row with the highest total count\n",
        "    idx = df.groupby('route')['total count'].idxmax()\n",
        "\n",
        "    # Get the unique routes with their maximum count\n",
        "    df_unique = df.loc[idx]\n",
        "\n",
        "    # Sort the results by 'total count' in descending order and take the top 20\n",
        "    top_20 = df_unique.sort_values('total count', ascending=False).head(top)\n",
        "\n",
        "    return top_20\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDNneKULM-O-",
        "outputId": "fb7af7e7-6b46-490d-c10c-326a45eddbc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing data: 82it [02:01,  2.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve data for 15-03-2025 on page 2. Status Code: 500\n",
            "TypeError: routesData.map is not a function\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing data: 116it [03:48,  1.87s/it]"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "city_df = pd.read_csv(\"mapping.csv\")\n",
        "# city_list = [\"Đà Nẵng\", \"Quảng Nam\", \"Quảng Ngãi\", \"Bình Định\", \"Phú Yên\", \"Khánh Hòa\", \"Ninh Thuận\", \"Bình Thuận\", \"Kon Tum\", \"Gia Lai\", \"Đắk Lắk\", \"Đắk Nông\", \"Lâm Đồng\", \"Bình Phước\", \"Bình Dương\", \"Đồng Nai\", \"Tây Ninh\", \"Bà Rịa - Vũng Tàu\", \"Hồ Chí Minh\", \"Long An\", \"Tiền Giang\", \"Bến Tre\", \"Trà Vinh\", \"Vĩnh Long\", \"Đồng Tháp\", \"An Giang\", \"Kiên Giang\", \"Cần Thơ\", \"Hậu Giang\", \"Sóc Trăng\", \"Bạc Liêu\", \"Cà Mau\"]\n",
        "city_list = city_df['name'].tolist()\n",
        "pairs = itertools.product(city_list, repeat=2)\n",
        "date = [\"15-03-2025\"]\n",
        "token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0eXAiOjIsInVzciI6ImZlIiwiY2lkIjoiYTRlYWM1MDAtMzYyNC0xMWU1LWFjOWUtMDkxMjRjNjAxMDEzIiwiZXhwIjoxNzQxMzYzODk0fQ.HuItYy9aH6txxwsOmG63IQbrp8keO-keBrQmOOb9TVw\"\n",
        "# Print pairs, skipping (i, i) pairs\n",
        "results = []\n",
        "df = pd.DataFrame(columns=['from destination', 'to destination', 'total count'])\n",
        "for pair in tqdm(pairs, desc=\"Processing data\"):\n",
        "  try:\n",
        "    if pair[0] != pair[1]:\n",
        "      data_ = get_bus_count(token,pair[0], pair[1], date)\n",
        "      if data_ is not None:\n",
        "        a, b, c, d = data_\n",
        "      else:\n",
        "        continue\n",
        "      df.loc[len(df)] = [a, b, c]  # Inserts at the next available row index\n",
        "      results.append(d)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    continue\n",
        "results = pd.concat(results, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYEy36uKM-O_"
      },
      "outputs": [],
      "source": [
        "# This is all the company name, number of count trip, for each date, for each route\n",
        "print(results)\n",
        "# Save the results to a CSV file\n",
        "# results.to_csv(\"results.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-c_nhlUM-O_"
      },
      "outputs": [],
      "source": [
        "# This is group by company name, sum the number of count trip, for all date, for all route\n",
        "# If you want to group by company name, delete the 'routes' in the list group_by_condition\n",
        "group_by_condition = ['Company Name']\n",
        "group_by_company = results.groupby(group_by_condition, as_index=False)['Count'].sum()\n",
        "sort_by_count = group_by_company.sort_values('Count', ascending=False).head(20)\n",
        "# Save the result to a CSV file\n",
        "# save_file = f\"result/group_by_company.csv\"\n",
        "# group_by_company.to_csv(save_file, index=False)\n",
        "# Save the result to a XLXS file\n",
        "sort_by_count.to_excel(\"result/group_by_company.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7g62nIxKM-PA"
      },
      "outputs": [],
      "source": [
        "# This return top best 20, 50 number of route with the highest number of count trip\n",
        "# If you want to change the number of top, change the 'top' in the function get_best\n",
        "top_best = get_best(df, top = 20)\n",
        "print(top_best)\n",
        "# Save the result to a CSV file\n",
        "# save_file = f\"result/top_best.csv\"\n",
        "# top_best.to_csv(save_file, index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}